import os
os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda'
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
os.environ['XLA_CLIENT_MEM_FRACTION'] = '0.8'  # ‰øÆÊ≠£: XLA_CLIENT_MEM_FRACTION

import subprocess

# find path for CUDA
try:
    nvcc_path = subprocess.check_output(['which', 'nvcc']).decode().strip()
    cuda_path = os.path.dirname(os.path.dirname(nvcc_path))
    print(f"CUDA„Éë„Çπ: {cuda_path}")
except:
    cuda_path = '/usr/local/cuda'  # default
    print(f"nvcc is not found, use CUDA: {cuda_path}")


from jax import grad, jit
import jax.numpy as jnp
from jax import random

# ========================================
# 1. Sharding configuration
# ========================================
import jax
print(f'Backend: {jax.default_backend()}')
print(f'Devices: {jax.devices()}')
devices = jax.devices()
from jax.sharding import PositionalSharding
sharding = PositionalSharding(devices) 

import numpyro
import numpyro.distributions as dist
from numpyro.infer import MCMC, NUTS

import numpy as np

import polars as pl

from tqdm import tqdm

from typing import Tuple, Optional, List
from dataclasses import dataclass

import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'Noto Sans CJK JP'
import seaborn as sns

import matplotlib.pyplot as plt
import matplotlib as mpl

# confirm fonts
from matplotlib.font_manager import FontManager
fm = FontManager()
fonts = [f.name for f in fm.ttflist]
print([f for f in fonts if 'Noto' in f or 'Sans' in f][:10])

# uses Noto Sans CJK JP
plt.rcParams['font.family'] = 'Noto Sans CJK JP'


import time
from functools import wraps

import os
# GPU optimization configurationÔºàrunning firstÔºâ
os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda'
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
os.environ['XLA_CLIENT_MEM_FRACTION'] = '0.8'




# ========================================
# 2. preprocessing
# ========================================
@jax.jit
def preprocess(data):
    """log transformation + standardization"""
    data_transformed = jnp.log1p(data)
    mean = data_transformed.mean(axis=0, keepdims=True)
    std = data_transformed.std(axis=0, keepdims=True)
    std = jnp.where(std < 1e-6, 1.0, std)
    return (data_transformed - mean) / std



def make_jax_error_matrix_gpu():

    from preprocess import process_complete_pipeline

    # from fastapi.responses import StreamingResponse
    from io import BytesIO
    import matplotlib
    matplotlib.use('Agg')  # for environment without GUI
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # getting dataset via somewhat preprocessing
    results = process_complete_pipeline(
        rawdata1_path="../data/rawdata1.csv",
        rawdata2_path="../data/rawdata2.csv",
        combined_path="../data/results.csv",
        display_results=False
    )
    
    df_error_matrix = results['error_matrix']
    
    # transform DataFrame to jax.numpy
    jax_error_matrix = jnp.array(df_error_matrix.to_numpy())

    jax_error_matrix_float32 = jax_error_matrix.astype(jnp.float32)
    jax_error_matrix_float32_standardized = preprocess(jax_error_matrix_float32)
    jax_error_matrix_gpu = jax.device_put(jax_error_matrix_float32_standardized, sharding.replicate())

    print(f"shape: {jax_error_matrix_gpu.shape}")
    print(f"type: {jax_error_matrix_gpu.dtype}")
    print(f"device: {jax_error_matrix_gpu.devices()}")

    return jax_error_matrix_gpu


# jax_error_matrix_gpu = make_jax_error_matrix_gpu()



# ========================================
# 3. (optimized) model building
# ========================================
def lkj_correlation_model(data, eta=1.0):
    """
    LKJÁõ∏Èñ¢Ë°åÂàóÊé®ÂÆö„É¢„Éá„É´ÔºàÊúÄÈÅ©ÂåñÁâàÔºâ
    
    Parameters:
    -----------
    data : array (n_samples, n_features)
        Ê®ôÊ∫ñÂåñÊ∏à„Åø„Éá„Éº„Çø
    eta : float
        LKJÊøÉÂ∫¶„Éë„É©„É°„Éº„ÇøÔºà1=‰∏ÄÊßò, >1=Âçò‰ΩçË°åÂàóÂØÑ„ÇäÔºâ
    """
    n_samples, n_features = data.shape
    
    # Cholesky decomposition (for corr matrix)
    L_corr = numpyro.sample(
        "L_corr", 
        dist.LKJCholesky(n_features, concentration=eta)
    )
    
    # stdÔºàExponential priorÔºâ
    sigma = numpyro.sample(
        "sigma", 
        dist.Exponential(1.0).expand([n_features])
    )
    
    # corr matrix
    corr_matrix = jnp.matmul(L_corr, L_corr.T)
    
    # cov matrix
    scale_matrix = jnp.diag(sigma)
    cov_matrix = scale_matrix @ corr_matrix @ scale_matrix
    cov_matrix = cov_matrix + jnp.eye(n_features) * 1e-3  # digit stabilization
    
    # average
    mu = numpyro.sample(
        "mu", 
        dist.Normal(0, 1).expand([n_features])
    )
    
    # likelihood
    with numpyro.plate("data", n_samples):
        numpyro.sample(
            "obs", 
            dist.MultivariateNormal(loc=mu, covariance_matrix=cov_matrix), 
            obs=data
        )


# ========================================
# 4. running MCMC
# ========================================
def mcmc_sampling():
    print("="*80)
    print("MCMC sampling")
    print("="*80)

    rng_key = random.PRNGKey(42)

    # NUTS configuration
    nuts_kernel = NUTS(
        lkj_correlation_model,
        target_accept_prob=0.9,
        max_tree_depth=10,
        init_strategy=numpyro.infer.init_to_median,
        regularize_mass_matrix=True
    )

    # MCMC configuration
    mcmc = MCMC(
        nuts_kernel,
        num_warmup=300,
        num_samples=700,
        num_chains=1,
        progress_bar=True
    )

    # running
    import time
    start_time = time.time()

    jax_error_matrix_gpu = make_jax_error_matrix_gpu()

    with jax.default_device(devices[0]):
        mcmc.run(rng_key, jax_error_matrix_gpu, eta=2.0)

    elapsed_time = time.time() - start_time

    print(f"\nrunning time: {elapsed_time:.2f}secs ({elapsed_time/60:.2f}mins)")
    print()

    return mcmc, elapsed_time


# ========================================
# 5. getting results and analysis
# ========================================
print("="*80)
print("results")
print("="*80)

mcmc, elapsed_time = mcmc_sampling()

samples = mcmc.get_samples()
L_corr_samples = samples["L_corr"]

# calcuratition for corr matrix
corr_matrices = jnp.matmul(
    L_corr_samples, 
    jnp.swapaxes(L_corr_samples, -2, -1)
)

# posterior stats
mean_corr = corr_matrices.mean(axis=0)
std_corr = corr_matrices.std(axis=0)
lower_ci = jnp.percentile(corr_matrices, 5, axis=0)
upper_ci = jnp.percentile(corr_matrices, 95, axis=0)

print(f"estimated corr matrixÔºàposterior averageÔºâ:")
print(mean_corr)
print()


# ========================================
# 6. diagnosis stats
# ========================================
print("="*80)
print("diagnosis stats")
print("="*80)
mcmc.print_summary(prob=0.9)
print()


# ========================================
# 7. Áõ∏Èñ¢„ÅÆË¶ÅÁ¥ÑÁµ±Ë®à
# ========================================
print("="*80)
print("Áõ∏Èñ¢‰øÇÊï∞„ÅÆË¶ÅÁ¥ÑÁµ±Ë®àÔºàÂØæËßíË¶ÅÁ¥†Èô§„ÅèÔºâ")
print("="*80)

off_diag_indices = jnp.triu_indices(mean_corr.shape[0], k=1)
off_diag_corrs = mean_corr[off_diag_indices]

print(f"Âπ≥Âùá: {off_diag_corrs.mean():.4f}")
print(f"‰∏≠Â§ÆÂÄ§: {jnp.median(off_diag_corrs):.4f}")
print(f"ÊúÄÂ∞è: {off_diag_corrs.min():.4f}")
print(f"ÊúÄÂ§ß: {off_diag_corrs.max():.4f}")
print(f"Ê®ôÊ∫ñÂÅèÂ∑Æ: {off_diag_corrs.std():.4f}")
print()

# Âº∑„ÅÑÁõ∏Èñ¢„ÅÆÊï∞
strong_positive = (off_diag_corrs > 0.5).sum()
strong_negative = (off_diag_corrs < -0.5).sum()
total_pairs = len(off_diag_corrs)

print(f"Âº∑„ÅÑÊ≠£„ÅÆÁõ∏Èñ¢ (r > 0.5): {strong_positive} / {total_pairs} ({strong_positive/total_pairs*100:.1f}%)")
print(f"Âº∑„ÅÑË≤†„ÅÆÁõ∏Èñ¢ (r < -0.5): {strong_negative} / {total_pairs} ({strong_negative/total_pairs*100:.1f}%)")
print()


# ========================================
# 8. ÁµêÊûú„ÅÆ‰øùÂ≠ò
# ========================================
results = {
    'mean_correlation': mean_corr,
    'std_correlation': std_corr,
    'lower_ci': lower_ci,
    'upper_ci': upper_ci,
    'samples': samples,
    'correlation_samples': corr_matrices,
    'execution_time': elapsed_time
}

print("="*80)
print("ÁµêÊûú„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà")
print("="*80)
print("results ËæûÊõ∏„Å´‰ª•‰∏ã„Åå‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Åæ„Åô:")
print("  - mean_correlation: ‰∫ãÂæåÂπ≥ÂùáÁõ∏Èñ¢Ë°åÂàó")
print("  - std_correlation: ‰∫ãÂæåÊ®ôÊ∫ñÂÅèÂ∑Æ")
print("  - lower_ci: 90%‰ø°Áî®Âå∫Èñì‰∏ãÈôê")
print("  - upper_ci: 90%‰ø°Áî®Âå∫Èñì‰∏äÈôê")
print("  - samples: ÂÖ®„Éë„É©„É°„Éº„Çø„ÅÆMCMC„Çµ„É≥„Éó„É´")
print("  - correlation_samples: Áõ∏Èñ¢Ë°åÂàó„ÅÆ„Çµ„É≥„Éó„É´")
print()


'''
ËøΩÂä†ÔºõÂ≠¶ÁøíÂæå„ÅÆMCMC„Çµ„É≥„Éó„É´„Åã„Çâ„ÅÆÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó„ÅÆË®àÁÆó
'''

# Ê¨°„ÅÆÈñ¢Êï∞„ÅÆÂÜÖÈÉ®„Åß‰Ωø„ÅÜÈñ¢Êï∞
def compute_covariance_matrix(sigma, L_corr):
    """
    Ê®ôÊ∫ñÂÅèÂ∑Æ„Å®CholeskyÂàÜËß£„Åï„Çå„ÅüÁõ∏Èñ¢Ë°åÂàó„Åã„ÇâÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó„ÇíË®àÁÆó
    
    Parameters:
    -----------
    sigma : array (n_features,)
        Ê®ôÊ∫ñÂÅèÂ∑Æ
    L_corr : array (n_features, n_features)
        Áõ∏Èñ¢Ë°åÂàó„ÅÆCholeskyÂàÜËß£
    
    Returns:
    --------
    cov_matrix : array (n_features, n_features)
        ÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó
    """
    # Áõ∏Èñ¢Ë°åÂàó„ÇíÂæ©ÂÖÉ: R = L * L^T
    corr_matrix = jnp.matmul(L_corr, L_corr.T)
    
    # „Çπ„Ç±„Éº„É´Ë°åÂàó(Ê®ôÊ∫ñÂÅèÂ∑Æ„ÅÆÂØæËßíË°åÂàó)
    scale_matrix = jnp.diag(sigma)
    
    # ÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó = D * R * D 
    # (D: Ê®ôÊ∫ñÂÅèÂ∑Æ„ÅÆÂØæËßíË°åÂàó, R: Áõ∏Èñ¢Ë°åÂàó)
    cov_matrix = scale_matrix @ corr_matrix @ scale_matrix
    
    return cov_matrix


def extract_covariance_from_mcmc_samples(samples):
    """
    MCMC„Çµ„É≥„Éó„É´„Åã„ÇâÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó„ÅÆ‰∫ãÂæåÂàÜÂ∏É„ÇíË®àÁÆó
    
    Parameters:
    -----------
    samples : dict
        mcmc.get_samples()„ÅÆÁµêÊûú
        ÂøÖË¶Å„Å™„Ç≠„Éº: 'sigma', 'L_corr'
    
    Returns:
    --------
    results : dict
        ‰ª•‰∏ã„ÅÆ„Ç≠„Éº„ÇíÂê´„ÇÄËæûÊõ∏:
        - mean_covariance: ‰∫ãÂæåÂπ≥ÂùáÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó
        - std_covariance: ÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó„ÅÆ‰∫ãÂæåÊ®ôÊ∫ñÂÅèÂ∑Æ
        - lower_ci_cov: 90%‰ø°Áî®Âå∫Èñì‰∏ãÈôê
        - upper_ci_cov: 90%‰ø°Áî®Âå∫Èñì‰∏äÈôê
        - covariance_samples: ÂÖ®„Çµ„É≥„Éó„É´„ÅÆÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó
        - variances: ÂêÑÁâπÂæ¥Èáè„ÅÆÂàÜÊï£(ÂØæËßíË¶ÅÁ¥†)
    """
    
    # „Çµ„É≥„Éó„É´„ÇíÂèñÂæó
    sigma_samples = samples["sigma"]      # shape: (n_samples, n_features)
    L_corr_samples = samples["L_corr"]    # shape: (n_samples, n_features, n_features)
    
    print(f"„Çµ„É≥„Éó„É´Êï∞: {sigma_samples.shape[0]}")
    print(f"ÁâπÂæ¥ÈáèÊï∞: {sigma_samples.shape[1]}")
    
    # „Éô„ÇØ„Éà„É´Âåñ„Åó„Å¶ÂÖ®„Çµ„É≥„Éó„É´„ÅÆÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó„ÇíË®àÁÆó
    print("\nÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó„ÇíË®àÁÆó‰∏≠...")
    cov_matrices = jax.vmap(compute_covariance_matrix)(sigma_samples, L_corr_samples)
    print(f"Ë®àÁÆóÂÆå‰∫Ü: shape = {cov_matrices.shape}")
    
    # ‰∫ãÂæåÁµ±Ë®àÈáè„ÇíË®àÁÆó
    mean_cov = cov_matrices.mean(axis=0)
    std_cov = cov_matrices.std(axis=0)
    lower_ci_cov = jnp.percentile(cov_matrices, 5, axis=0)
    upper_ci_cov = jnp.percentile(cov_matrices, 95, axis=0)
    
    # ÂàÜÊï£(ÂØæËßíË¶ÅÁ¥†)„ÇíÊäΩÂá∫
    variances = jnp.diagonal(mean_cov)
    
    # ÁµêÊûú„ÇíËæûÊõ∏„Å´„Åæ„Å®„ÇÅ„Çã
    results = {
        'mean_covariance': mean_cov,
        'std_covariance': std_cov,
        'lower_ci_cov': lower_ci_cov,
        'upper_ci_cov': upper_ci_cov,
        'covariance_samples': cov_matrices,
        'variances': variances
    }
    
    return results


# visualize
def print_covariance_summary(results, show_heatmap=True, figsize=(16, 7), save_path=None):
# def print_covariance_summary(results):
    """
    ÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó„ÅÆÁµêÊûú„ÇíË°®Á§∫
    
    Parameters:
    -----------
    results : dict
        extract_covariance_from_mcmc_samples()„ÅÆÁµêÊûú
    show_heatmap : bool
        „Éí„Éº„Éà„Éû„ÉÉ„Éó„ÇíË°®Á§∫„Åô„Çã„Åã„Å©„ÅÜ„Åã („Éá„Éï„Ç©„É´„Éà: True)
    figsize : tuple
        Âõ≥„ÅÆ„Çµ„Ç§„Ç∫ („Éá„Éï„Ç©„É´„Éà: (16, 7))
    save_path : str or None
        „Éí„Éº„Éà„Éû„ÉÉ„Éó„ÅÆ‰øùÂ≠òÂÖà„Éë„Çπ (None„ÅÆÂ†¥Âêà„ÅØ‰øùÂ≠ò„Åó„Å™„ÅÑ)
    """
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    print("\n" + "="*80)
    print("ÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó„ÅÆÁµêÊûú")
    print("="*80)
    
    mean_cov = results['mean_covariance']
    variances = results['variances']
    
    print(f"\nÊé®ÂÆöÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó(‰∫ãÂæåÂπ≥Âùá):")
    print(mean_cov)
    
    print(f"\nÂêÑÁâπÂæ¥Èáè„ÅÆÂàÜÊï£(ÂØæËßíË¶ÅÁ¥†):")
    print(variances)
    
    print(f"\nÂàÜÊï£„ÅÆË¶ÅÁ¥ÑÁµ±Ë®àÈáè:")
    print(f"  Âπ≥Âùá: {variances.mean():.4f}")
    print(f"  ‰∏≠Â§ÆÂÄ§: {jnp.median(variances):.4f}")
    print(f"  ÊúÄÂ∞è: {variances.min():.4f}")
    print(f"  ÊúÄÂ§ß: {variances.max():.4f}")
    print(f"  Ê®ôÊ∫ñÂÅèÂ∑Æ: {variances.std():.4f}")
    
    # ÂÖ±ÂàÜÊï£(ÈùûÂØæËßíË¶ÅÁ¥†)„ÅÆÁµ±Ë®à
    n = mean_cov.shape[0]
    off_diag_indices = jnp.triu_indices(n, k=1)
    off_diag_covs = mean_cov[off_diag_indices]
    
    print(f"\nÂÖ±ÂàÜÊï£(ÈùûÂØæËßíË¶ÅÁ¥†)„ÅÆË¶ÅÁ¥ÑÁµ±Ë®àÈáè:")
    print(f"  Âπ≥Âùá: {off_diag_covs.mean():.4f}")
    print(f"  ‰∏≠Â§ÆÂÄ§: {jnp.median(off_diag_covs):.4f}")
    print(f"  ÊúÄÂ∞è: {off_diag_covs.min():.4f}")
    print(f"  ÊúÄÂ§ß: {off_diag_covs.max():.4f}")
    print(f"  Ê®ôÊ∫ñÂÅèÂ∑Æ: {off_diag_covs.std():.4f}")
    
    # „Éí„Éº„Éà„Éû„ÉÉ„Éó„ÅÆË°®Á§∫
    if show_heatmap:
        print("\n" + "="*80)
        print("ÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó„ÅÆ„Éí„Éº„Éà„Éû„ÉÉ„Éó")
        print("="*80)
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
        
        # 1. ÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó(‰∫ãÂæåÂπ≥Âùá)
        sns.heatmap(mean_cov, 
                    annot=False, 
                    cmap='RdBu_r', 
                    center=0, 
                    square=True, 
                    ax=ax1,
                    cbar_kws={'label': 'Covariance'})
        ax1.set_title('Covariance Matrix (Posterior Mean)', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Feature Index', fontsize=12)
        ax1.set_ylabel('Feature Index', fontsize=12)
        
        # 2. ÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó(‰∫ãÂæåÊ®ôÊ∫ñÂÅèÂ∑Æ) - ‰∏çÁ¢∫ÂÆüÊÄß
        if 'std_covariance' in results:
            std_cov = results['std_covariance']
            sns.heatmap(std_cov, 
                        annot=False, 
                        cmap='viridis', 
                        square=True, 
                        ax=ax2,
                        cbar_kws={'label': 'Standard Deviation'})
            ax2.set_title('Covariance Uncertainty (Posterior Std)', fontsize=14, fontweight='bold')
            ax2.set_xlabel('Feature Index', fontsize=12)
            ax2.set_ylabel('Feature Index', fontsize=12)
        else:
            # std_covariance„Åå„Å™„ÅÑÂ†¥Âêà„ÅØÂàÜÊï£„ÅÆ„Éê„Éº„Éó„É≠„ÉÉ„Éà„ÇíË°®Á§∫
            ax2.bar(range(len(variances)), variances, edgecolor='black', alpha=0.7)
            ax2.set_xlabel('Feature Index', fontsize=12)
            ax2.set_ylabel('Variance', fontsize=12)
            ax2.set_title('Variance per Feature', fontsize=14, fontweight='bold')
            ax2.grid(alpha=0.3, axis='y')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"\n„Éí„Éº„Éà„Éû„ÉÉ„Éó„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: {save_path}")
        
        plt.show()
        print("\n„Éí„Éº„Éà„Éû„ÉÉ„Éó„ÇíË°®Á§∫„Åó„Åæ„Åó„Åü„ÄÇ")
    


# ========================================
# ‰ΩøÁî®‰æã
# ========================================
if __name__ == "__main__":

    # ÂÖÉ„ÅÆlkj.py„ÅßMCMC„Çµ„É≥„Éó„É™„É≥„Ç∞„ÇíÂÆüË°å„Åó„ÅüÂæå„ÄÅ
    # ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´„Åó„Å¶ÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó„ÇíÂèñÂæó„Åß„Åç„Åæ„Åô:
    
    # MCMC„Çµ„É≥„Éó„É™„É≥„Ç∞Âæå
    samples = mcmc.get_samples()
    
    # ÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó„ÇíË®àÁÆó
    # from covariance_calculation import extract_covariance_from_mcmc_samples, print_covariance_summary
    
    cov_results = extract_covariance_from_mcmc_samples(samples)
    print_covariance_summary(cov_results)
    
    # ÁµêÊûú„ÇíÂÖÉ„ÅÆresultsËæûÊõ∏„Å´ËøΩÂä†
    results.update(cov_results)
    
    # ‰ΩøÁî®ÊñπÊ≥ï:
    # - cov_results['mean_covariance']  # ‰∫ãÂæåÂπ≥ÂùáÂàÜÊï£ÂÖ±ÂàÜÊï£Ë°åÂàó
    # - cov_results['variances']        # ÂêÑÁâπÂæ¥Èáè„ÅÆÂàÜÊï£
    # - cov_results['covariance_samples']  # ÂÖ®„Çµ„É≥„Éó„É´(„Éô„Ç§„Ç∫Êé®ÂÆö„ÅÆ‰∏çÁ¢∫ÂÆüÊÄßË©ï‰æ°„Å´‰ΩøÁî®)
    
    print(__doc__)




# ========================================
# 9. ÂèØË¶ñÂåñ„Ç≥„Éº„Éâ
# ========================================
# print("="*80)
# print("ÂèØË¶ñÂåñÔºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ")
# print("="*80)
# print("""
# import matplotlib.pyplot as plt
# import seaborn as sns

# # Áõ∏Èñ¢Ë°åÂàó„ÅÆ„Éí„Éº„Éà„Éû„ÉÉ„Éó
# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))

# # ‰∫ãÂæåÂπ≥Âùá
# sns.heatmap(results['mean_correlation'], annot=False, cmap='coolwarm', 
#             center=0, vmin=-1, vmax=1, square=True, ax=ax1,
#             cbar_kws={'label': 'Correlation'})
# ax1.set_title('Estimated Correlation Matrix (Posterior Mean)', fontsize=14)
# ax1.set_xlabel('Error Type', fontsize=12)
# ax1.set_ylabel('Error Type', fontsize=12)

# # ‰∫ãÂæåÊ®ôÊ∫ñÂÅèÂ∑Æ
# sns.heatmap(results['std_correlation'], annot=False, cmap='viridis', 
#             square=True, ax=ax2, cbar_kws={'label': 'Standard Deviation'})
# ax2.set_title('Posterior Standard Deviation', fontsize=14)
# ax2.set_xlabel('Error Type', fontsize=12)
# ax2.set_ylabel('Error Type', fontsize=12)

# plt.tight_layout()
# plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')
# plt.show()

# # Áõ∏Èñ¢‰øÇÊï∞„ÅÆÂàÜÂ∏É
# fig, ax = plt.subplots(figsize=(10, 6))
# ax.hist(off_diag_corrs, bins=50, edgecolor='black', alpha=0.7)
# ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero correlation')
# ax.set_xlabel('Correlation Coefficient', fontsize=12)
# ax.set_ylabel('Frequency', fontsize=12)
# ax.set_title('Distribution of Pairwise Correlations', fontsize=14)
# ax.legend()
# ax.grid(alpha=0.3)
# plt.tight_layout()
# plt.savefig('correlation_distribution.png', dpi=300, bbox_inches='tight')
# plt.show()
# """)

# print("\nÂá¶ÁêÜÂÆå‰∫ÜÔºÅ üéâ")

# ```

# **„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉ:**
# ```
# ÂÆüË£Ö                          ÂÆüË°åÊôÇÈñì        ÈÄüÂ∫¶ÊØî
# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
# PyMC (CPU)                    ~600Áßí (10ÂàÜ)   1.0x
# JAX/NumPyro (CPU)             ~150Áßí (2.5ÂàÜ)  4.0x
# JAX/NumPyro (GPU, ÊúÄÈÅ©ÂåñÂâç)   ~660Áßí (11ÂàÜ)   0.9x ‚ùå
# JAX/NumPyro (GPU, ÊúÄÈÅ©ÂåñÂæå)    21Áßí (0.35ÂàÜ)  28.6x ‚úÖ